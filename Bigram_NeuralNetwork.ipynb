{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "7d0a02c1-67db-48c1-852a-7f5206754138",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('Names.txt', 'r').read().splitlines()\n",
    "words = [x.lower() for x in words]\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['<>'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "fa39d9a2-0475-4497-b0ab-7a5ec0f86410",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the training sets for the bigram neural network\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words[:1]:\n",
    "    chs = ['<>'] + list(w) + ['<>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        x1 = stoi[ch1]\n",
    "        x2 = stoi[ch2]\n",
    "        xs.append(x1)\n",
    "        ys.append(x2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "036a75f7-1830-4405-8677-5f454fa6b8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(9845165659)\n",
    "W = torch.randn((27,27), generator = g)\n",
    "\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "logits = xenc @ W\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "6d259474-f1ed-4d80-9ca8-30203c3c9ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*---------------*\n",
      "Bigram example 1: <>l (indexes 0, 12)\n",
      "Input to the neural net = 0\n",
      "Output probabilities from the neural net: tensor([0.0041, 0.0358, 0.0143, 0.0129, 0.0369, 0.0173, 0.0201, 0.0922, 0.0386,\n",
      "        0.0026, 0.0700, 0.0177, 0.0206, 0.0151, 0.1510, 0.0316, 0.0117, 0.0061,\n",
      "        0.0411, 0.0630, 0.0050, 0.0251, 0.0124, 0.0555, 0.0034, 0.1864, 0.0095])\n",
      "Actual next character = 12\n",
      "Probability assigned by the neural net to the next actual character: 0.020574\n",
      "Negative log likelihood: 3.883740\n",
      "*---------------*\n",
      "Bigram example 2: li (indexes 12, 9)\n",
      "Input to the neural net = 12\n",
      "Output probabilities from the neural net: tensor([0.0217, 0.0066, 0.0301, 0.0273, 0.0028, 0.0153, 0.0330, 0.0645, 0.1567,\n",
      "        0.0352, 0.0256, 0.0076, 0.0116, 0.0357, 0.0503, 0.1076, 0.0339, 0.0065,\n",
      "        0.1230, 0.0327, 0.0500, 0.0073, 0.0342, 0.0216, 0.0247, 0.0232, 0.0112])\n",
      "Actual next character = 9\n",
      "Probability assigned by the neural net to the next actual character: 0.035201\n",
      "Negative log likelihood: 3.346683\n",
      "*---------------*\n",
      "Bigram example 3: ia (indexes 9, 1)\n",
      "Input to the neural net = 9\n",
      "Output probabilities from the neural net: tensor([0.0406, 0.0139, 0.0803, 0.0233, 0.0561, 0.0809, 0.0092, 0.0522, 0.0623,\n",
      "        0.0139, 0.0020, 0.0171, 0.0119, 0.0424, 0.1236, 0.0120, 0.0864, 0.0265,\n",
      "        0.0602, 0.0173, 0.0534, 0.0309, 0.0056, 0.0146, 0.0282, 0.0205, 0.0146])\n",
      "Actual next character = 1\n",
      "Probability assigned by the neural net to the next actual character: 0.013926\n",
      "Negative log likelihood: 4.273982\n",
      "*---------------*\n",
      "Bigram example 4: am (indexes 1, 13)\n",
      "Input to the neural net = 1\n",
      "Output probabilities from the neural net: tensor([0.2745, 0.0855, 0.0071, 0.0135, 0.0098, 0.0260, 0.0419, 0.0079, 0.0654,\n",
      "        0.0145, 0.0293, 0.0058, 0.0665, 0.0290, 0.0062, 0.0023, 0.0120, 0.0223,\n",
      "        0.0350, 0.0800, 0.0066, 0.0210, 0.0478, 0.0042, 0.0290, 0.0076, 0.0493])\n",
      "Actual next character = 13\n",
      "Probability assigned by the neural net to the next actual character: 0.029046\n",
      "Negative log likelihood: 3.538869\n",
      "*---------------*\n",
      "Bigram example 5: m<> (indexes 13, 0)\n",
      "Input to the neural net = 13\n",
      "Output probabilities from the neural net: tensor([0.0039, 0.0173, 0.0223, 0.0146, 0.0118, 0.2783, 0.0131, 0.0038, 0.0164,\n",
      "        0.0485, 0.0335, 0.0116, 0.1939, 0.0062, 0.0189, 0.0152, 0.0283, 0.0070,\n",
      "        0.0080, 0.0580, 0.0298, 0.0364, 0.0300, 0.0165, 0.0050, 0.0460, 0.0259])\n",
      "Actual next character = 0\n",
      "Probability assigned by the neural net to the next actual character: 0.003884\n",
      "Negative log likelihood: 5.551003\n",
      "Average negative log likelihood: 4.1188554763793945\n"
     ]
    }
   ],
   "source": [
    "itos = {i:s for s,i in stoi.items()}\n",
    "nlls = torch.zeros(5)\n",
    "for i in range (5):\n",
    "    x = xs[i].item()\n",
    "    y = ys[i].item()\n",
    "    print (\"*---------------*\")\n",
    "    print (f'Bigram example {i+1}: {itos[x]}{itos[y]} (indexes {x}, {y})')\n",
    "    print (f'Input to the neural net = {x}')\n",
    "    print (f'Output probabilities from the neural net:', probs[i])\n",
    "    print (f'Actual next character = {y}')\n",
    "    print (f'Probability assigned by the neural net to the next actual character: {probs[i,y].item():4f}')\n",
    "    logprob = torch.log(probs[i,y])\n",
    "    nll = -logprob\n",
    "    print (f'Negative log likelihood: {nll.item():4f}')\n",
    "    nlls[i] = nll\n",
    "\n",
    "print (f'Average negative log likelihood: {nlls.mean().item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "id": "4cb25def-9141-43ba-b039-5dfd12c5d57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********* Start of the Neural Network Implementation Of The Above Bigram Model *********"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 903,
   "id": "1fbf0b41-fcff-4533-b603-d36d6f7820a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of input samples: 13576\n"
     ]
    }
   ],
   "source": [
    "#creating the training sets for the bigram neural network\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    chs = ['<>'] + list(w) + ['<>']\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        x1 = stoi[ch1]\n",
    "        x2 = stoi[ch2]\n",
    "        xs.append(x1)\n",
    "        ys.append(x2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print(f'Total number of input samples: {num}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 904,
   "id": "67416ab0-2f60-4ab9-8622-f008c0ecbd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random initialisation of the weights of the neurons for the first run (ONLY FIRST TIME!)\n",
    "g = torch.Generator().manual_seed(9845165659)\n",
    "W = torch.randn((27,27), generator = g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 916,
   "id": "9a8e8e64-a986-4665-8304-666b3867f40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss=: 2.4055187702178955\n",
      "Loss=: 2.405505895614624\n",
      "Loss=: 2.4054927825927734\n",
      "Loss=: 2.405480146408081\n",
      "Loss=: 2.4054670333862305\n",
      "Loss=: 2.405454397201538\n",
      "Loss=: 2.4054415225982666\n",
      "Loss=: 2.405428409576416\n",
      "Loss=: 2.4054157733917236\n",
      "Loss=: 2.4054031372070312\n",
      "Loss=: 2.405390501022339\n",
      "Loss=: 2.4053776264190674\n",
      "Loss=: 2.405364990234375\n",
      "Loss=: 2.4053523540496826\n",
      "Loss=: 2.4053397178649902\n",
      "Loss=: 2.4053268432617188\n",
      "Loss=: 2.4053146839141846\n",
      "Loss=: 2.405301809310913\n",
      "Loss=: 2.405289649963379\n",
      "Loss=: 2.4052767753601074\n",
      "Loss=: 2.4052646160125732\n",
      "Loss=: 2.40525221824646\n",
      "Loss=: 2.4052395820617676\n",
      "Loss=: 2.405226945877075\n",
      "Loss=: 2.405214786529541\n",
      "Loss=: 2.4052023887634277\n",
      "Loss=: 2.4051902294158936\n",
      "Loss=: 2.4051778316497803\n",
      "Loss=: 2.405165433883667\n",
      "Loss=: 2.405153274536133\n",
      "Loss=: 2.4051411151885986\n",
      "Loss=: 2.4051289558410645\n",
      "Loss=: 2.405116558074951\n",
      "Loss=: 2.405104398727417\n",
      "Loss=: 2.405092239379883\n",
      "Loss=: 2.4050800800323486\n",
      "Loss=: 2.4050681591033936\n",
      "Loss=: 2.4050559997558594\n",
      "Loss=: 2.405043840408325\n",
      "Loss=: 2.40503191947937\n",
      "Loss=: 2.405019760131836\n",
      "Loss=: 2.405007839202881\n",
      "Loss=: 2.404995918273926\n",
      "Loss=: 2.40498423576355\n",
      "Loss=: 2.4049720764160156\n",
      "Loss=: 2.4049601554870605\n",
      "Loss=: 2.4049484729766846\n",
      "Loss=: 2.4049365520477295\n",
      "Loss=: 2.4049246311187744\n",
      "Loss=: 2.4049129486083984\n",
      "Loss=: 2.4049012660980225\n",
      "Loss=: 2.4048893451690674\n",
      "Loss=: 2.4048779010772705\n",
      "Loss=: 2.4048662185668945\n",
      "Loss=: 2.4048545360565186\n",
      "Loss=: 2.4048428535461426\n",
      "Loss=: 2.4048309326171875\n",
      "Loss=: 2.4048194885253906\n",
      "Loss=: 2.4048078060150146\n",
      "Loss=: 2.4047961235046387\n",
      "Loss=: 2.404784679412842\n",
      "Loss=: 2.404773473739624\n",
      "Loss=: 2.404761791229248\n",
      "Loss=: 2.404750108718872\n",
      "Loss=: 2.4047389030456543\n",
      "Loss=: 2.4047272205352783\n",
      "Loss=: 2.4047160148620605\n",
      "Loss=: 2.4047045707702637\n",
      "Loss=: 2.404693126678467\n",
      "Loss=: 2.40468168258667\n",
      "Loss=: 2.4046707153320312\n",
      "Loss=: 2.4046595096588135\n",
      "Loss=: 2.4046478271484375\n",
      "Loss=: 2.4046366214752197\n",
      "Loss=: 2.404625654220581\n",
      "Loss=: 2.404614210128784\n",
      "Loss=: 2.4046030044555664\n",
      "Loss=: 2.4045920372009277\n",
      "Loss=: 2.404580593109131\n",
      "Loss=: 2.404569625854492\n",
      "Loss=: 2.4045584201812744\n",
      "Loss=: 2.4045474529266357\n",
      "Loss=: 2.404536485671997\n",
      "Loss=: 2.4045252799987793\n",
      "Loss=: 2.4045140743255615\n",
      "Loss=: 2.404503345489502\n",
      "Loss=: 2.404492139816284\n",
      "Loss=: 2.4044814109802246\n",
      "Loss=: 2.404470443725586\n",
      "Loss=: 2.4044594764709473\n",
      "Loss=: 2.4044485092163086\n",
      "Loss=: 2.404437780380249\n",
      "Loss=: 2.4044265747070312\n",
      "Loss=: 2.404416084289551\n",
      "Loss=: 2.404404878616333\n",
      "Loss=: 2.4043943881988525\n",
      "Loss=: 2.404383420944214\n",
      "Loss=: 2.4043726921081543\n",
      "Loss=: 2.4043619632720947\n",
      "Loss=: 2.404351234436035\n"
     ]
    }
   ],
   "source": [
    "# Gradient descent optimisation of the neural network\n",
    "\n",
    "for Optimisation in range (100):\n",
    "\n",
    "    # Forward pass of the neural network\n",
    "    import torch.nn.functional as F\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim = True)\n",
    "    loss = - probs[torch.arange(num), ys].log().mean()\n",
    "    print(f'Loss=: {loss}')\n",
    "    \n",
    "    # Backward pass of the neural netword\n",
    "    W.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Finetuning the weights of the neural netword\n",
    "    learning_rate = 10\n",
    "    W.data += -learning_rate * W.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 917,
   "id": "a98be0eb-c94c-4741-8b2a-308f06b0dd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ka<>\n",
      "ahute<>\n",
      "gethan<>\n",
      "verinian<>\n",
      "ryan<>\n",
      "kyman<>\n",
      "hleruuttelen<>\n",
      "h<>\n",
      "ke<>\n",
      "kyniovelilesalirqesieriaderl<>\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the Neural Network\n",
    "\n",
    "g = torch.Generator().manual_seed(666)\n",
    "for i in range (10):\n",
    "    out = []\n",
    "    index = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([index]), num_classes=27).float()\n",
    "        logits = xenc @ W\n",
    "        counts = logits.exp()\n",
    "        probs = counts / counts.sum(1, keepdims = True)\n",
    "    \n",
    "        index = torch.multinomial(probs, num_samples = 1, replacement = True, generator = g).item()\n",
    "        out.append(itos[index])\n",
    "        if index == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7aeb3-7d54-4856-bd66-ad212e2a6534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
